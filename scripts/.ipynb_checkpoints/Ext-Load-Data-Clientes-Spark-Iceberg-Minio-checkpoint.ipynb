{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a731398-3a91-468c-9a88-cbf4c2a2f0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerías generales\n",
    "#=========================\n",
    "import json\n",
    "from json import dumps\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lit, explode, from_json, col, current_date\n",
    "from pyspark.sql.types import StringType, StructType, StructField, ArrayType\n",
    "import datetime \n",
    "import requests\n",
    "import urllib3\n",
    "from datetime import datetime,date\n",
    "import pytz\n",
    "import io\n",
    "import glob\n",
    "import os\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d0dd369-21f5-4f9f-ba94-2a17e79bfa16",
   "metadata": {},
   "outputs": [],
   "source": [
    "CATALOG_URI = \"http://nessie:19120/api/v1\"  # Nessie Server URI\n",
    "WAREHOUSE = \"s3://gold/\"               # Minio Address to Write to\n",
    "STORAGE_URI = \"http://172.20.0.5:9000\"      # Minio IP address from docker inspect\n",
    "AWS_ACCESS_KEY='admin'\n",
    "AWS_SECRET_KEY='password'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89dedad1-f5c1-49ad-8ed1-248491bbfe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de Minio\n",
    "minio_client = boto3.client(\n",
    "    's3',\n",
    "    #endpoint_url='http://172.18.0.4:9000',  # Usar el nombre del servicio de Docker\n",
    "    endpoint_url='http://minio:9000',\n",
    "    aws_access_key_id='admin',\n",
    "    aws_secret_access_key='password',\n",
    "    region_name='us-east-1'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6fb7346-3ee3-4f16-8d62-850bde651ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark\n",
    "\n",
    "# Configuración combinada\n",
    "conf = (\n",
    "    pyspark.SparkConf()\n",
    "        .setAppName('combined_spark_app')\n",
    "        # Paquetes para PostgreSQL, Iceberg, Nessie, AWS SDK y Hadoop AWS\n",
    "        .set('spark.jars.packages', 'org.postgresql:postgresql:42.7.3,'\n",
    "                                    'org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.0,'\n",
    "                                    'org.projectnessie.nessie-integrations:nessie-spark-extensions-3.5_2.12:0.77.1,'\n",
    "                                    'software.amazon.awssdk:bundle:2.24.8,'\n",
    "                                    'software.amazon.awssdk:url-connection-client:2.24.8,'\n",
    "                                    'org.apache.hadoop:hadoop-aws:3.2.0,'\n",
    "                                    'com.amazonaws:aws-java-sdk-bundle:1.11.534')\n",
    "        # Extensiones de Iceberg y Nessie\n",
    "        .set('spark.sql.extensions', 'org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions,'\n",
    "                                     'org.projectnessie.spark.extensions.NessieSparkSessionExtensions')\n",
    "        # Configuración de Nessie como catálogo Iceberg\n",
    "        .set('spark.sql.catalog.nessie', 'org.apache.iceberg.spark.SparkCatalog')\n",
    "        .set('spark.sql.catalog.nessie.uri', CATALOG_URI)\n",
    "        .set('spark.sql.catalog.nessie.ref', 'main')\n",
    "        .set('spark.sql.catalog.nessie.authentication.type', 'NONE')\n",
    "        .set('spark.sql.catalog.nessie.catalog-impl', 'org.apache.iceberg.nessie.NessieCatalog')\n",
    "        # Configuración para almacenamiento en S3 (MinIO)\n",
    "        .set('spark.sql.catalog.nessie.s3.endpoint', STORAGE_URI)\n",
    "        .set('spark.sql.catalog.nessie.warehouse', WAREHOUSE)\n",
    "        .set('spark.sql.catalog.nessie.io-impl', 'org.apache.iceberg.aws.s3.S3FileIO')\n",
    "        # Configuración para acceso a S3 directamente desde Spark (sin Nessie)\n",
    "        .set('spark.hadoop.fs.s3a.endpoint', STORAGE_URI)\n",
    "        .set('spark.hadoop.fs.s3a.access.key', AWS_ACCESS_KEY)\n",
    "        .set('spark.hadoop.fs.s3a.secret.key', AWS_SECRET_KEY)\n",
    "        .set('spark.hadoop.fs.s3a.path.style.access', 'true')\n",
    "        \n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36935c46-245b-40b2-95f2-29b7475295d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session Started\n"
     ]
    }
   ],
   "source": [
    "# Creación de la sesión de Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=conf) \\\n",
    "    .getOrCreate()\n",
    "print(\"Spark Session Started\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6499e5a3-ca07-4b76-8b25-e3932cd3634f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sodapy in /opt/conda/lib/python3.11/site-packages (2.2.0)\n",
      "Requirement already satisfied: requests>=2.28.1 in /opt/conda/lib/python3.11/site-packages (from sodapy) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests>=2.28.1->sodapy) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests>=2.28.1->sodapy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests>=2.28.1->sodapy) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests>=2.28.1->sodapy) (2023.7.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Requests made without an app_token will be subject to strict throttling limits.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# make sure to install these packages before running:\n",
    "# pip install pandas\n",
    "!pip install sodapy\n",
    "from sodapy import Socrata\n",
    "\n",
    "# Unauthenticated client only works with public data sets. Note 'None'\n",
    "# in place of application token, and no username or password:\n",
    "client = Socrata(\"www.datos.gov.co\", None)\n",
    "\n",
    "# Example authenticated client (needed for non-public datasets):\n",
    "# client = Socrata(www.datos.gov.co,\n",
    "#                  MyAppToken,\n",
    "#                  username=\"user@example.com\",\n",
    "#                  password=\"AFakePassword\")\n",
    "\n",
    "# First 2000 results, returned as JSON from API / converted to Python list of\n",
    "# dictionaries by sodapy.\n",
    "results = client.get(\"vafm-j2df\", limit=2000)\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df_divi = pd.DataFrame.from_records(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab50a0bb-37ef-48d7-b95c-74005f9f0c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cod_dpto</th>\n",
       "      <th>nom_dpto</th>\n",
       "      <th>cod_mpio</th>\n",
       "      <th>nom_mpio</th>\n",
       "      <th>tipo</th>\n",
       "      <th>latitud</th>\n",
       "      <th>longitud</th>\n",
       "      <th>geo_municipio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>ANTIOQUIA</td>\n",
       "      <td>5001</td>\n",
       "      <td>MEDELLÍN</td>\n",
       "      <td>Municipio</td>\n",
       "      <td>6.257590259</td>\n",
       "      <td>-75.61103107</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-75.61103107...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>ANTIOQUIA</td>\n",
       "      <td>5002</td>\n",
       "      <td>ABEJORRAL</td>\n",
       "      <td>Municipio</td>\n",
       "      <td>5.803728154</td>\n",
       "      <td>-75.43847353</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-75.43847353...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>ANTIOQUIA</td>\n",
       "      <td>5004</td>\n",
       "      <td>ABRIAQUÍ</td>\n",
       "      <td>Municipio</td>\n",
       "      <td>6.627569378</td>\n",
       "      <td>-76.08597756</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-76.08597756...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>ANTIOQUIA</td>\n",
       "      <td>5021</td>\n",
       "      <td>ALEJANDRÍA</td>\n",
       "      <td>Municipio</td>\n",
       "      <td>6.365534125</td>\n",
       "      <td>-75.09059702</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-75.09059702...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>ANTIOQUIA</td>\n",
       "      <td>5030</td>\n",
       "      <td>AMAGÁ</td>\n",
       "      <td>Municipio</td>\n",
       "      <td>6.032921994</td>\n",
       "      <td>-75.7080031</td>\n",
       "      <td>{'type': 'Point', 'coordinates': [-75.7080031,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cod_dpto   nom_dpto cod_mpio    nom_mpio       tipo      latitud  \\\n",
       "0        5  ANTIOQUIA     5001    MEDELLÍN  Municipio  6.257590259   \n",
       "1        5  ANTIOQUIA     5002   ABEJORRAL  Municipio  5.803728154   \n",
       "2        5  ANTIOQUIA     5004    ABRIAQUÍ  Municipio  6.627569378   \n",
       "3        5  ANTIOQUIA     5021  ALEJANDRÍA  Municipio  6.365534125   \n",
       "4        5  ANTIOQUIA     5030       AMAGÁ  Municipio  6.032921994   \n",
       "\n",
       "       longitud                                      geo_municipio  \n",
       "0  -75.61103107  {'type': 'Point', 'coordinates': [-75.61103107...  \n",
       "1  -75.43847353  {'type': 'Point', 'coordinates': [-75.43847353...  \n",
       "2  -76.08597756  {'type': 'Point', 'coordinates': [-76.08597756...  \n",
       "3  -75.09059702  {'type': 'Point', 'coordinates': [-75.09059702...  \n",
       "4   -75.7080031  {'type': 'Point', 'coordinates': [-75.7080031,...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_divi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd5ed88b-155a-46af-a2a0-cb7ab7765611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1121 entries, 0 to 1120\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   cod_dpto       1121 non-null   object\n",
      " 1   nom_dpto       1121 non-null   object\n",
      " 2   cod_mpio       1121 non-null   object\n",
      " 3   nom_mpio       1121 non-null   object\n",
      " 4   tipo           1121 non-null   object\n",
      " 5   latitud        1121 non-null   object\n",
      " 6   longitud       1121 non-null   object\n",
      " 7   geo_municipio  1121 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 70.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_divi.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd9f8400-3024-443c-acf4-580593a58252",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_divi_sparkDF=spark.createDataFrame(df_divi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d7ab014-2ad0-404d-84b5-0a6dc340146f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+--------+----------+---------+-----------+------------+--------------------+\n",
      "|cod_dpto| nom_dpto|cod_mpio|  nom_mpio|     tipo|    latitud|    longitud|       geo_municipio|\n",
      "+--------+---------+--------+----------+---------+-----------+------------+--------------------+\n",
      "|       5|ANTIOQUIA|    5001|  MEDELLÍN|Municipio|6.257590259|-75.61103107|{coordinates -> [...|\n",
      "|       5|ANTIOQUIA|    5002| ABEJORRAL|Municipio|5.803728154|-75.43847353|{coordinates -> [...|\n",
      "|       5|ANTIOQUIA|    5004|  ABRIAQUÍ|Municipio|6.627569378|-76.08597756|{coordinates -> [...|\n",
      "|       5|ANTIOQUIA|    5021|ALEJANDRÍA|Municipio|6.365534125|-75.09059702|{coordinates -> [...|\n",
      "|       5|ANTIOQUIA|    5030|     AMAGÁ|Municipio|6.032921994| -75.7080031|{coordinates -> [...|\n",
      "|       5|ANTIOQUIA|    5031|    AMALFI|Municipio|6.977788843| -74.9812393|{coordinates -> [...|\n",
      "|       5|ANTIOQUIA|    5034|     ANDES|Municipio|5.604993248|-75.94128391|{coordinates -> [...|\n",
      "+--------+---------+--------+----------+---------+-----------+------------+--------------------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_divi_sparkDF.show(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08243c9f-0f60-4360-8c08-afd3e80f1fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Eliminar espacios en los nombres de las columnas y convertirlas a mayúsculas\n",
    "df_divi_sparkDF = df_divi_sparkDF.toDF(*[c.strip().replace(' ', '_').upper() for c in df_divi_sparkDF.columns])\n",
    "\n",
    "# Eliminar la columna 'GEO_MUNICIPIO'\n",
    "df_divi_sparkDF = df_divi_sparkDF.drop('GEO_MUNICIPIO')\n",
    "\n",
    "# Convertir LATITUD y LONGITUD a tipo float\n",
    "df_divi_sparkDF = df_divi_sparkDF.withColumn('LATITUD', col('LATITUD').cast('double'))\n",
    "df_divi_sparkDF = df_divi_sparkDF.withColumn('LONGITUD', col('LONGITUD').cast('double'))\n",
    "\n",
    "# Filtrar las filas donde NOM_DPTO sea 'VALLE DEL CAUCA'\n",
    "df_divi_sparkDF = df_divi_sparkDF.filter(col('NOM_DPTO') == 'VALLE DEL CAUCA')\n",
    "\n",
    "# Eliminar la columna 'TIPO'\n",
    "df_divi_sparkDF = df_divi_sparkDF.drop('TIPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2977149e-8cc9-43f0-85fd-5555f9a2b587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+--------+------------+-----------+------------+\n",
      "|COD_DPTO|       NOM_DPTO|COD_MPIO|    NOM_MPIO|    LATITUD|    LONGITUD|\n",
      "+--------+---------------+--------+------------+-----------+------------+\n",
      "|      76|VALLE DEL CAUCA|   76001|        CALI|3.399043723|-76.57649259|\n",
      "|      76|VALLE DEL CAUCA|   76020|      ALCALÁ|  4.6788971|-75.78297932|\n",
      "|      76|VALLE DEL CAUCA|   76036|   ANDALUCÍA|4.153314228|-76.16063341|\n",
      "|      76|VALLE DEL CAUCA|   76041|ANSERMANUEVO|4.795927292|-76.02963049|\n",
      "|      76|VALLE DEL CAUCA|   76054|     ARGELIA|4.704287864|-76.14164999|\n",
      "+--------+---------------+--------+------------+-----------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_divi_sparkDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "57aa542d-6dd8-4e29-81ad-f746aa71eb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import urllib3\n",
    "from io import StringIO\n",
    "\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "url_instalaciones = \"https://raw.githubusercontent.com/ouzka/webinar-uao/refs/heads/main/Data_EDA.csv\"\n",
    "response = requests.get(url_instalaciones)\n",
    "print(response)\n",
    "if response.status_code == 200:\n",
    "    csv_data = StringIO(response.text)\n",
    "    df_inst = pd.read_csv(csv_data,encoding= 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07dc43b4-3fd4-4ccc-bbbf-0f1430697388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DEPARTAMENTO</th>\n",
       "      <th>CIUDAD</th>\n",
       "      <th>SERIAL</th>\n",
       "      <th>PRODUCTO</th>\n",
       "      <th>FECHA INSTALACION</th>\n",
       "      <th>FECHA TERMINACION</th>\n",
       "      <th>ESTADO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>VALLE DEL CAUCA</td>\n",
       "      <td>PALMIRA</td>\n",
       "      <td>3002071.0</td>\n",
       "      <td>Producto-Oro</td>\n",
       "      <td>5/03/2020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Con servicio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TOLIMA</td>\n",
       "      <td>IBAGUÉ</td>\n",
       "      <td>3194094.0</td>\n",
       "      <td>Producto-Plata</td>\n",
       "      <td>15/08/2023</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Con servicio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VALLE DEL CAUCA</td>\n",
       "      <td>PALMIRA</td>\n",
       "      <td>3021114.0</td>\n",
       "      <td>Producto-Plata</td>\n",
       "      <td>26/02/2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Con servicio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>VALLE DEL CAUCA</td>\n",
       "      <td>PALMIRA</td>\n",
       "      <td>3001881.0</td>\n",
       "      <td>Producto-Bronce</td>\n",
       "      <td>12/05/2022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Con servicio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>VALLE DEL CAUCA</td>\n",
       "      <td>GUADALAJARA DE BUGA</td>\n",
       "      <td>3262138.0</td>\n",
       "      <td>Producto-Plata</td>\n",
       "      <td>16/05/2024</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Con servicio</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      DEPARTAMENTO               CIUDAD     SERIAL         PRODUCTO  \\\n",
       "0  VALLE DEL CAUCA              PALMIRA  3002071.0     Producto-Oro   \n",
       "1           TOLIMA               IBAGUÉ  3194094.0   Producto-Plata   \n",
       "2  VALLE DEL CAUCA              PALMIRA  3021114.0   Producto-Plata   \n",
       "3  VALLE DEL CAUCA              PALMIRA  3001881.0  Producto-Bronce   \n",
       "4  VALLE DEL CAUCA  GUADALAJARA DE BUGA  3262138.0   Producto-Plata   \n",
       "\n",
       "  FECHA INSTALACION FECHA TERMINACION        ESTADO  \n",
       "0         5/03/2020               NaN  Con servicio  \n",
       "1        15/08/2023               NaN  Con servicio  \n",
       "2        26/02/2024               NaN  Con servicio  \n",
       "3        12/05/2022               NaN  Con servicio  \n",
       "4        16/05/2024               NaN  Con servicio  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inst.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9870c1f3-07d0-406f-b01f-7c7a5942cfce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 293 entries, 0 to 292\n",
      "Data columns (total 7 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   DEPARTAMENTO       293 non-null    object \n",
      " 1   CIUDAD             293 non-null    object \n",
      " 2   SERIAL             291 non-null    float64\n",
      " 3   PRODUCTO           293 non-null    object \n",
      " 4   FECHA INSTALACION  293 non-null    object \n",
      " 5   FECHA TERMINACION  20 non-null     object \n",
      " 6   ESTADO             293 non-null    object \n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 16.2+ KB\n"
     ]
    }
   ],
   "source": [
    "df_inst.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51e95190-0e0b-445e-a186-dd6558f373ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inst_sparkDF=spark.createDataFrame(df_inst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3f1e0ed-df3e-4b87-93d0-8b539292fa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+---------+---------------+-----------------+-----------------+------------+\n",
      "|   DEPARTAMENTO|             CIUDAD|   SERIAL|       PRODUCTO|FECHA INSTALACION|FECHA TERMINACION|      ESTADO|\n",
      "+---------------+-------------------+---------+---------------+-----------------+-----------------+------------+\n",
      "|VALLE DEL CAUCA|            PALMIRA|3002071.0|   Producto-Oro|        5/03/2020|              NaN|Con servicio|\n",
      "|         TOLIMA|             IBAGUÉ|3194094.0| Producto-Plata|       15/08/2023|              NaN|Con servicio|\n",
      "|VALLE DEL CAUCA|            PALMIRA|3021114.0| Producto-Plata|       26/02/2024|              NaN|Con servicio|\n",
      "|VALLE DEL CAUCA|            PALMIRA|3001881.0|Producto-Bronce|       12/05/2022|              NaN|Con servicio|\n",
      "|VALLE DEL CAUCA|GUADALAJARA DE BUGA|3262138.0| Producto-Plata|       16/05/2024|              NaN|Con servicio|\n",
      "+---------------+-------------------+---------+---------------+-----------------+-----------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_inst_sparkDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cdafb219-9ad6-4b5c-9c71-f780dbf5c8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-------+---------------+-----------------+-----------------+------------+\n",
      "|   DEPARTAMENTO|             CIUDAD| SERIAL|       PRODUCTO|FECHA_INSTALACION|FECHA_TERMINACION|      ESTADO|\n",
      "+---------------+-------------------+-------+---------------+-----------------+-----------------+------------+\n",
      "|VALLE DEL CAUCA|            PALMIRA|3002071|   Producto-Oro|             NULL|             NULL|Con servicio|\n",
      "|VALLE DEL CAUCA|            PALMIRA|3021114| Producto-Plata|             NULL|             NULL|Con servicio|\n",
      "|VALLE DEL CAUCA|            PALMIRA|3001881|Producto-Bronce|             NULL|             NULL|Con servicio|\n",
      "|VALLE DEL CAUCA|GUADALAJARA DE BUGA|3262138| Producto-Plata|             NULL|             NULL|Con servicio|\n",
      "|VALLE DEL CAUCA|         EL CERRITO|3105407|Producto-Bronce|             NULL|             NULL|Con servicio|\n",
      "|VALLE DEL CAUCA|            PALMIRA|3012492|   Producto-Oro|             NULL|             NULL|Con servicio|\n",
      "|VALLE DEL CAUCA|         EL CERRITO|3364951| Producto-Plata|             NULL|             NULL|Con servicio|\n",
      "|VALLE DEL CAUCA|            PALMIRA|3260402| Producto-Plata|             NULL|             NULL|Con servicio|\n",
      "|VALLE DEL CAUCA|GUADALAJARA DE BUGA|3261794|   Producto-Oro|             NULL|             NULL|Con servicio|\n",
      "|VALLE DEL CAUCA|            PALMIRA|3020835|   Producto-Oro|             NULL|             NULL|Con servicio|\n",
      "+---------------+-------------------+-------+---------------+-----------------+-----------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, to_date, date_format\n",
    "\n",
    "# Normalizar nombres de columnas\n",
    "df_inst_sparkDF = df_inst_sparkDF.toDF(*[c.strip().replace(' ', '_').upper() for c in df_inst_sparkDF.columns])\n",
    "\n",
    "# Reformatear fechas para asegurarse de que tengan dos dígitos en día y mes antes de la conversión\n",
    "df_inst_sparkDF = df_inst_sparkDF.withColumn(\"FECHA_INSTALACION\", \n",
    "    to_date(date_format(col(\"FECHA_INSTALACION\"), \"dd/MM/yyyy\"), \"dd/MM/yyyy\"))\n",
    "\n",
    "df_inst_sparkDF = df_inst_sparkDF.withColumn(\"FECHA_TERMINACION\", \n",
    "    to_date(date_format(col(\"FECHA_TERMINACION\"), \"dd/MM/yyyy\"), \"dd/MM/yyyy\"))\n",
    "\n",
    "# Reemplazar valores nulos en PRODUCTO\n",
    "df_inst_sparkDF = df_inst_sparkDF.withColumn('PRODUCTO', when(col('PRODUCTO').isNull(), 'NO_ESPECIFICA').otherwise(col('PRODUCTO')))\n",
    "\n",
    "# Reemplazar valores nulos en SERIAL y convertir a entero\n",
    "df_inst_sparkDF = df_inst_sparkDF.withColumn('SERIAL', when(col('SERIAL').isNull(), 0).otherwise(col('SERIAL').cast('int')))\n",
    "\n",
    "# Filtrar por DEPARTAMENTO = 'VALLE DEL CAUCA'\n",
    "df_inst_sparkDF = df_inst_sparkDF.filter(col('DEPARTAMENTO') == 'VALLE DEL CAUCA')\n",
    "\n",
    "# Mostrar resultados\n",
    "df_inst_sparkDF.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "545bf4fc-60bb-4095-94e4-b216b4b0866b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-------------------+-------+---------------+-----------------+-----------------+------------+\n",
      "|   DEPARTAMENTO|             CIUDAD| SERIAL|       PRODUCTO|FECHA_INSTALACION|FECHA_TERMINACION|      ESTADO|\n",
      "+---------------+-------------------+-------+---------------+-----------------+-----------------+------------+\n",
      "|VALLE DEL CAUCA|            PALMIRA|3002071|   Producto-Oro|             NULL|             NULL|Con servicio|\n",
      "|VALLE DEL CAUCA|            PALMIRA|3021114| Producto-Plata|             NULL|             NULL|Con servicio|\n",
      "|VALLE DEL CAUCA|            PALMIRA|3001881|Producto-Bronce|             NULL|             NULL|Con servicio|\n",
      "|VALLE DEL CAUCA|GUADALAJARA DE BUGA|3262138| Producto-Plata|             NULL|             NULL|Con servicio|\n",
      "|VALLE DEL CAUCA|         EL CERRITO|3105407|Producto-Bronce|             NULL|             NULL|Con servicio|\n",
      "|VALLE DEL CAUCA|            PALMIRA|3012492|   Producto-Oro|             NULL|             NULL|Con servicio|\n",
      "|VALLE DEL CAUCA|         EL CERRITO|3364951| Producto-Plata|             NULL|             NULL|Con servicio|\n",
      "|VALLE DEL CAUCA|            PALMIRA|3260402| Producto-Plata|             NULL|             NULL|Con servicio|\n",
      "|VALLE DEL CAUCA|GUADALAJARA DE BUGA|3261794|   Producto-Oro|             NULL|             NULL|Con servicio|\n",
      "|VALLE DEL CAUCA|            PALMIRA|3020835|   Producto-Oro|             NULL|             NULL|Con servicio|\n",
      "+---------------+-------------------+-------+---------------+-----------------+-----------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_inst_sparkDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66e88369-78f5-4863-ac67-d4040ab97441",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Realizar la unión entre df_inst y df_divi\n",
    "df_merge_sparkDF = df_inst_sparkDF.join(df_divi_sparkDF.select('NOM_MPIO', 'LATITUD', 'LONGITUD'), \n",
    "                        df_inst_sparkDF.CIUDAD == df_divi_sparkDF.NOM_MPIO, \n",
    "                        how='inner')\n",
    "\n",
    "# Eliminar la columna 'NOM_MPIO'\n",
    "df_merge_sparkDF = df_merge_sparkDF.drop('NOM_MPIO')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7fdbd8b4-6646-4ed9-b021-477a67ce8752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+-------+--------------+-----------------+-----------------+------------+-----------+------------+\n",
      "|   DEPARTAMENTO|      CIUDAD| SERIAL|      PRODUCTO|FECHA_INSTALACION|FECHA_TERMINACION|      ESTADO|    LATITUD|    LONGITUD|\n",
      "+---------------+------------+-------+--------------+-----------------+-----------------+------------+-----------+------------+\n",
      "|VALLE DEL CAUCA|     BOLÍVAR|3304183|  Producto-Oro|             NULL|             NULL|Con servicio|4.393118861|-76.34982039|\n",
      "|VALLE DEL CAUCA|BUENAVENTURA|3267784|Producto-Plata|             NULL|             NULL|Con servicio|3.493340766|-77.11872832|\n",
      "|VALLE DEL CAUCA|BUGALAGRANDE|3345424|Producto-Plata|             NULL|             NULL|Con servicio|4.196852991| -76.0896103|\n",
      "|VALLE DEL CAUCA|  CANDELARIA|3326217|  Producto-Oro|             NULL|             NULL|Con servicio|3.382091564|-76.38317663|\n",
      "|VALLE DEL CAUCA|  CANDELARIA|3092356|  Producto-Oro|             NULL|             NULL|Con servicio|3.382091564|-76.38317663|\n",
      "|VALLE DEL CAUCA|  CANDELARIA|3306747|  Producto-Oro|             NULL|             NULL|Con servicio|3.382091564|-76.38317663|\n",
      "+---------------+------------+-------+--------------+-----------------+-----------------+------------+-----------+------------+\n",
      "only showing top 6 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_merge_sparkDF.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cb8871c9-0ef3-44fc-a1c1-94dc295118a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "\n",
    "# Agregar la columna 'f_actual' con la fecha actual\n",
    "df_inst_divi_sparkDF = df_merge_sparkDF.withColumn('f_actual', current_date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3201dcfb-e057-4dcc-902d-4828dbbbc169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------------+-------+--------------+-----------------+-----------------+------------+-----------+------------+----------+\n",
      "|   DEPARTAMENTO|      CIUDAD| SERIAL|      PRODUCTO|FECHA_INSTALACION|FECHA_TERMINACION|      ESTADO|    LATITUD|    LONGITUD|  f_actual|\n",
      "+---------------+------------+-------+--------------+-----------------+-----------------+------------+-----------+------------+----------+\n",
      "|VALLE DEL CAUCA|     BOLÍVAR|3304183|  Producto-Oro|             NULL|             NULL|Con servicio|4.393118861|-76.34982039|2025-09-24|\n",
      "|VALLE DEL CAUCA|BUENAVENTURA|3267784|Producto-Plata|             NULL|             NULL|Con servicio|3.493340766|-77.11872832|2025-09-24|\n",
      "|VALLE DEL CAUCA|BUGALAGRANDE|3345424|Producto-Plata|             NULL|             NULL|Con servicio|4.196852991| -76.0896103|2025-09-24|\n",
      "+---------------+------------+-------+--------------+-----------------+-----------------+------------+-----------+------------+----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_inst_divi_sparkDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a06186fe-fc61-42c5-a44f-777c0d998ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo subido: part-00000-32d96294-ce8f-4099-8d85-f678c05a76e1-c000.snappy.parquet → bronze/clientes_inst/part-00000-32d96294-ce8f-4099-8d85-f678c05a76e1-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "# Ruta temporal donde Spark guarda los archivos Parquet\n",
    "temp_folder_path = \"/tmp/df_inst_divi_sparkDF_parquet\"\n",
    "\n",
    "# Guardar en formato Parquet sin consolidar (múltiples archivos)\n",
    "df_inst_divi_sparkDF.write.mode(\"overwrite\").parquet(temp_folder_path)\n",
    "\n",
    "# Buscar todos los archivos Parquet generados en la carpeta\n",
    "parquet_files = glob.glob(f\"{temp_folder_path}/*.parquet\")\n",
    "\n",
    "# Definir bucket y prefijo dentro del bucket\n",
    "bucket_name = \"bronze\"  # Asegurar que el bucket no tenga /\n",
    "folder_prefix = \"clientes_inst/\"  # Prefijo dentro del bucket\n",
    "\n",
    "if parquet_files:\n",
    "    for file_path in parquet_files:\n",
    "        file_name = os.path.basename(file_path)  # Obtener solo el nombre del archivo\n",
    "        object_name = folder_prefix + file_name  # Agregar el prefijo de \"carpeta\"\n",
    "\n",
    "        # Subir cada archivo Parquet a MinIO con la estructura correcta\n",
    "        minio_client.upload_file(file_path, bucket_name, object_name)\n",
    "        print(f\"✅ Archivo subido: {file_name} → {bucket_name}/{object_name}\")\n",
    "else:\n",
    "    print(\"⚠️ Error: No se encontraron archivos Parquet para subir.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f4082ad-d483-4e14-be85-37abb0bee8db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEPARTAMENTO: string (nullable = true)\n",
      " |-- CIUDAD: string (nullable = true)\n",
      " |-- SERIAL: integer (nullable = true)\n",
      " |-- PRODUCTO: string (nullable = true)\n",
      " |-- FECHA_INSTALACION: date (nullable = true)\n",
      " |-- FECHA_TERMINACION: date (nullable = true)\n",
      " |-- ESTADO: string (nullable = true)\n",
      " |-- LATITUD: double (nullable = true)\n",
      " |-- LONGITUD: double (nullable = true)\n",
      " |-- f_actual: date (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_inst_divi_sparkDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e61d9a8-faac-4360-afad-e7894f1ed2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"CREATE NAMESPACE IF NOT EXISTS nessie.gold;\").show()\n",
    "#SOLO UNA VEZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54fa39d9-c93b-4359-829a-7b2cfe2c260d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.sql(\"DROP NAMESPACE IF EXISTS nessie.bronze CASCADE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "05f6f14e-79c8-4c72-a967-0b5d1b774d68",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o165.createOrReplace.\n: org.apache.iceberg.exceptions.NoSuchNamespaceException: Namespace does not exist: gold\n\tat org.apache.iceberg.nessie.NessieUtil.maybeUseSpecializedException(NessieUtil.java:302)\n\tat org.apache.iceberg.nessie.NessieUtil.handleExceptionsForCommits(NessieUtil.java:224)\n\tat org.apache.iceberg.nessie.NessieTableOperations.doCommit(NessieTableOperations.java:125)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.commit(BaseMetastoreTableOperations.java:135)\n\tat org.apache.iceberg.BaseTransaction.lambda$commitReplaceTransaction$1(BaseTransaction.java:381)\n\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n\tat org.apache.iceberg.BaseTransaction.commitReplaceTransaction(BaseTransaction.java:365)\n\tat org.apache.iceberg.BaseTransaction.commitTransaction(BaseTransaction.java:314)\n\tat org.apache.iceberg.CommitCallbackTransaction.commitTransaction(CommitCallbackTransaction.java:126)\n\tat org.apache.iceberg.spark.source.StagedSparkTable.commitStagedChanges(StagedSparkTable.java:34)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:580)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:573)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:567)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:183)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:216)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:196)\n\tat org.apache.spark.sql.DataFrameWriterV2.internalReplace(DataFrameWriterV2.scala:208)\n\tat org.apache.spark.sql.DataFrameWriterV2.createOrReplace(DataFrameWriterV2.scala:134)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.projectnessie.error.NessieReferenceConflictException: Namespace 'gold' must exist.\n\tat org.projectnessie.error.ErrorCode.lambda$asException$1(ErrorCode.java:66)\n\tat java.base/java.util.Optional.map(Optional.java:260)\n\tat org.projectnessie.error.ErrorCode.asException(ErrorCode.java:66)\n\tat org.projectnessie.client.rest.ResponseCheckFilter.checkResponse(ResponseCheckFilter.java:58)\n\tat org.projectnessie.client.rest.NessieHttpResponseFilter.filter(NessieHttpResponseFilter.java:29)\n\tat org.projectnessie.client.http.impl.jdk11.JavaRequest.lambda$executeRequest$1(JavaRequest.java:143)\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\n\tat java.base/java.util.Collections$UnmodifiableCollection.forEach(Collections.java:1092)\n\tat org.projectnessie.client.http.impl.jdk11.JavaRequest.executeRequest(JavaRequest.java:143)\n\tat org.projectnessie.client.http.HttpRequest.post(HttpRequest.java:116)\n\tat org.projectnessie.client.rest.v1.RestV1TreeClient.commitMultipleOperations(RestV1TreeClient.java:204)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat org.projectnessie.client.rest.v1.RestV1Client$ExceptionRewriter.invoke(RestV1Client.java:84)\n\tat jdk.proxy3/jdk.proxy3.$Proxy41.commitMultipleOperations(Unknown Source)\n\tat org.projectnessie.client.rest.v1.HttpCommitMultipleOperations.commit(HttpCommitMultipleOperations.java:34)\n\tat org.apache.iceberg.nessie.NessieIcebergClient.commitContent(NessieIcebergClient.java:687)\n\tat org.apache.iceberg.nessie.NessieIcebergClient.commitTable(NessieIcebergClient.java:619)\n\tat org.apache.iceberg.nessie.NessieTableOperations.doCommit(NessieTableOperations.java:120)\n\t... 55 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_inst_divi_sparkDF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriteTo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnessie.gold.clientes_inst\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateOrReplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/readwriter.py:2100\u001b[0m, in \u001b[0;36mDataFrameWriterV2.createOrReplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2090\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;241m3.1\u001b[39m)\n\u001b[1;32m   2091\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreateOrReplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2093\u001b[0m \u001b[38;5;124;03m    Create a new table or replace an existing table with the contents of the data frame.\u001b[39;00m\n\u001b[1;32m   2094\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2098\u001b[0m \u001b[38;5;124;03m    If the table exists, its configuration and data will be replaced.\u001b[39;00m\n\u001b[1;32m   2099\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2100\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateOrReplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o165.createOrReplace.\n: org.apache.iceberg.exceptions.NoSuchNamespaceException: Namespace does not exist: gold\n\tat org.apache.iceberg.nessie.NessieUtil.maybeUseSpecializedException(NessieUtil.java:302)\n\tat org.apache.iceberg.nessie.NessieUtil.handleExceptionsForCommits(NessieUtil.java:224)\n\tat org.apache.iceberg.nessie.NessieTableOperations.doCommit(NessieTableOperations.java:125)\n\tat org.apache.iceberg.BaseMetastoreTableOperations.commit(BaseMetastoreTableOperations.java:135)\n\tat org.apache.iceberg.BaseTransaction.lambda$commitReplaceTransaction$1(BaseTransaction.java:381)\n\tat org.apache.iceberg.util.Tasks$Builder.runTaskWithRetry(Tasks.java:413)\n\tat org.apache.iceberg.util.Tasks$Builder.runSingleThreaded(Tasks.java:219)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:203)\n\tat org.apache.iceberg.util.Tasks$Builder.run(Tasks.java:196)\n\tat org.apache.iceberg.BaseTransaction.commitReplaceTransaction(BaseTransaction.java:365)\n\tat org.apache.iceberg.BaseTransaction.commitTransaction(BaseTransaction.java:314)\n\tat org.apache.iceberg.CommitCallbackTransaction.commitTransaction(CommitCallbackTransaction.java:126)\n\tat org.apache.iceberg.spark.source.StagedSparkTable.commitStagedChanges(StagedSparkTable.java:34)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.$anonfun$writeToTable$1(WriteToDataSourceV2Exec.scala:580)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable(WriteToDataSourceV2Exec.scala:573)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CreateTableAsSelectBaseExec.writeToTable$(WriteToDataSourceV2Exec.scala:567)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.writeToTable(WriteToDataSourceV2Exec.scala:183)\n\tat org.apache.spark.sql.execution.datasources.v2.AtomicReplaceTableAsSelectExec.run(WriteToDataSourceV2Exec.scala:216)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n\tat org.apache.spark.sql.DataFrameWriterV2.runCommand(DataFrameWriterV2.scala:196)\n\tat org.apache.spark.sql.DataFrameWriterV2.internalReplace(DataFrameWriterV2.scala:208)\n\tat org.apache.spark.sql.DataFrameWriterV2.createOrReplace(DataFrameWriterV2.scala:134)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.projectnessie.error.NessieReferenceConflictException: Namespace 'gold' must exist.\n\tat org.projectnessie.error.ErrorCode.lambda$asException$1(ErrorCode.java:66)\n\tat java.base/java.util.Optional.map(Optional.java:260)\n\tat org.projectnessie.error.ErrorCode.asException(ErrorCode.java:66)\n\tat org.projectnessie.client.rest.ResponseCheckFilter.checkResponse(ResponseCheckFilter.java:58)\n\tat org.projectnessie.client.rest.NessieHttpResponseFilter.filter(NessieHttpResponseFilter.java:29)\n\tat org.projectnessie.client.http.impl.jdk11.JavaRequest.lambda$executeRequest$1(JavaRequest.java:143)\n\tat java.base/java.util.ArrayList.forEach(ArrayList.java:1511)\n\tat java.base/java.util.Collections$UnmodifiableCollection.forEach(Collections.java:1092)\n\tat org.projectnessie.client.http.impl.jdk11.JavaRequest.executeRequest(JavaRequest.java:143)\n\tat org.projectnessie.client.http.HttpRequest.post(HttpRequest.java:116)\n\tat org.projectnessie.client.rest.v1.RestV1TreeClient.commitMultipleOperations(RestV1TreeClient.java:204)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat org.projectnessie.client.rest.v1.RestV1Client$ExceptionRewriter.invoke(RestV1Client.java:84)\n\tat jdk.proxy3/jdk.proxy3.$Proxy41.commitMultipleOperations(Unknown Source)\n\tat org.projectnessie.client.rest.v1.HttpCommitMultipleOperations.commit(HttpCommitMultipleOperations.java:34)\n\tat org.apache.iceberg.nessie.NessieIcebergClient.commitContent(NessieIcebergClient.java:687)\n\tat org.apache.iceberg.nessie.NessieIcebergClient.commitTable(NessieIcebergClient.java:619)\n\tat org.apache.iceberg.nessie.NessieTableOperations.doCommit(NessieTableOperations.java:120)\n\t... 55 more\n"
     ]
    }
   ],
   "source": [
    "df_inst_divi_sparkDF.writeTo(\"nessie.gold.clientes_inst\").createOrReplace()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753d94f1-dd25-4970-8e44-a93cfe1a0acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_inst_divi_sparkDF.write \\\n",
    "#    .format(\"iceberg\") \\\n",
    "#    .mode(\"overwrite\") \\\n",
    "#    .save(\"nessie.gold.clientes_inst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5365aca-0221-4f35-8293-040c76d8ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.table(\"nessie.gold.clientes_inst\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2587be23-86ff-4122-826d-e5043ac4c50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec2bc58-8fa0-40f9-a858-1a651a78b1cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
