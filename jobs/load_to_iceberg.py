import logging
from pyspark.sql import SparkSession
import boto3

# ==============================
# Logging
# ==============================
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ==============================
# Validar bucket en MinIO
# ==============================
def ensure_bucket_exists(endpoint, access_key, secret_key, bucket_name="gold"):
    logger.info(f"ğŸ” Verificando bucket `{bucket_name}` en MinIO...")
    s3 = boto3.client(
        "s3",
        endpoint_url=endpoint,
        aws_access_key_id=access_key,
        aws_secret_access_key=secret_key,
        region_name="us-east-1",
    )
    buckets = [b["Name"] for b in s3.list_buckets()["Buckets"]]
    if bucket_name not in buckets:
        s3.create_bucket(Bucket=bucket_name)
        logger.info(f"ğŸ†• Bucket `{bucket_name}` creado en MinIO")
    else:
        logger.info(f"âœ… Bucket `{bucket_name}` ya existe en MinIO")

# ==============================
# Validar tabla Iceberg
# ==============================
def drop_table_if_exists(spark, table_name="nessie.gold.yellow_tripdata"):
    try:
        namespace = table_name.split(".")[1]
        table = table_name.split(".")[2]
        tables = [t.name for t in spark.sql(f"SHOW TABLES IN {namespace}").collect()]
        if table in tables:
            logger.info(f"âš ï¸ Tabla {table_name} existe pero archivos pueden estar corruptos. Eliminando...")
            spark.sql(f"DROP TABLE {table_name}")
            logger.info(f"âœ… Tabla {table_name} eliminada")
        else:
            logger.info(f"âœ… Tabla {table_name} no existe, se puede crear")
    except Exception as e:
        logger.warning(f"âŒ No se pudo verificar la tabla: {e}")

# ==============================
# Lectura de datos
# ==============================
def read_data(spark):
    try:
        logger.info("ğŸ“¥ Leyendo parquet desde Bronze...")
        df = spark.read.parquet("s3a://bronze/yellow_tripdata_2025-01.parquet")
        logger.info(f"âœ”ï¸ Datos leÃ­dos: {df.count()} registros")
        df.printSchema()
        return df
    except Exception as e:
        logger.error(f"âŒ Error en lectura: {e}")
        raise

# ==============================
# Escritura en Iceberg
# ==============================
def write_to_iceberg(spark, df):
    try:
        logger.info("ğŸ”§ Creando namespace si no existe...")
        spark.sql("CREATE NAMESPACE IF NOT EXISTS nessie.gold")

        logger.info("ğŸ’¾ Escribiendo en Iceberg (nessie.gold.yellow_tripdata)...")
        df.writeTo("nessie.gold.yellow_tripdata").createOrReplace()  # o .append() segÃºn tu lÃ³gica

        logger.info("ğŸ‰ Escritura completada")
    except Exception as e:
        logger.error(f"âŒ Error en escritura: {e}")
        raise

# ==============================
# Main
# ==============================
def main():
    spark = SparkSession.builder.appName("LoadToIceberg").getOrCreate()
    logger.info("ğŸš€ SparkSession iniciada")

    try:
        # Validar bucket en MinIO
        ensure_bucket_exists(
            endpoint="http://minio:9000",
            access_key="admin",
            secret_key="password"
        )

        # ETL
        df = read_data(spark)

        # Validar y eliminar tabla Iceberg si existÃ­a
        drop_table_if_exists(spark, "nessie.gold.yellow_tripdata")

        # Escritura
        write_to_iceberg(spark, df)

    finally:
        spark.stop()
        logger.info("ğŸ›‘ SparkSession detenida")

if __name__ == "__main__":
    main()
